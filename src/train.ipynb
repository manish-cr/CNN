{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.optim\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import dataset\n",
    "import src.model_store as model_store\n",
    "\n",
    "\n",
    "# store hyper parameters\n",
    "hparams  = {\n",
    "    \"detaset\": \"CIFAR-10\", \n",
    "    \"optimizer\": \"SGD\", \n",
    "    \"momentum\": 0., \n",
    "    \"epochs\": 10, \n",
    "    \"train_batch_size\": 4, \n",
    "    \"eval_batch_size\": 4, \n",
    "    \"lr\": 1e-3, \n",
    "    \"checkpoint\": 1000, \n",
    "}\n",
    "\n",
    "# avalable GPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# get dataset\n",
    "train, test = dataset.get_dataset(hparams[\"detaset\"])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                train, \n",
    "                batch_size=hparams[\"train_batch_size\"], \n",
    "                shuffle=False, \n",
    "                num_workers=2)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                test, \n",
    "                batch_size=hparams[\"eval_batch_size\"], \n",
    "                shuffle=False, \n",
    "                num_workers=2)\n",
    "\n",
    "\n",
    "# get model\n",
    "model = model_store.get_model(hparams).to(device)\n",
    "\n",
    "# get optimizer\n",
    "if hparams[\"optimizer\"] == \"SGD\":\n",
    "    optimizer = torch.optim.SGD(model.parameters(), hparams[\"lr\"], momentum=hparams[\"momentum\"])\n",
    "elif hparams[\"optimizer\"] == \"Adam\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(), hparams[\"lr\"])\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Training Started\")\n",
    "# training phase\n",
    "for epoch in range(hparams[\"epochs\"]):\n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        images, labes = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(model(images), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % hparams[\"checkpoint\"] == 0 or i == len(train_loader) - 1:\n",
    "            print(f\"epoch{epoch + 1} - {i + 1}steps loss: {loss}\")\n",
    "            \n",
    "                    \n",
    "print(\"Training Done\")\n",
    "\n",
    "print(\"Evaluation Started\")\n",
    "# test phase\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        logits = model(images)\n",
    "        pred_label = logits.argmax(dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (pred_label == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
