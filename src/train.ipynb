{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "BnUXEW5pfhLW",
        "outputId": "0dd495b8-e8be-4de6-92c0-f71c5d552c6c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from torch.autograd import Variable\n",
        "import torch.optim\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import dataset\n",
        "import model_store\n",
        "\n",
        "!pip install wandb -qU\n",
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "OPTIMIZERS = [\"SGD\", \"Momentum SGD\", \"Adam\"]\n",
        "LR = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
        "BATCH_SIZE = [4, 8, 16, 32, 64]\n",
        "MOMENTUM = [0., 0.5, 0.9]\n",
        "SCHEDULER = [\"Vanilla\", \"LambdaLR\", \"StepLR\", \"CosineAnnealingLR\"]\n",
        "\n",
        "for optim in OPTIMIZERS:\n",
        "  for lr in LR:\n",
        "    \n",
        "    # store hyper parameters\n",
        "    hparams  = {\n",
        "        \"model\": \"CNN\",\n",
        "        \"detaset\": \"CIFAR-10\",\n",
        "        \"optimizer\": optim,\n",
        "        \"momentum\": 0.,\n",
        "        \"epochs\": 5,\n",
        "        \"train_batch_size\": 8,\n",
        "        \"eval_batch_size\": 32,\n",
        "        \"lr\": lr,\n",
        "        \"checkpoint\": 100,\n",
        "    }\n",
        "\n",
        "    if optim == \"Momentum SGD\":\n",
        "      hparams[\"momentum\"] = 0.9\n",
        "\n",
        "    train_bs = hparams[\"train_batch_size\"]\n",
        "    run_id = f\"{optim}_{lr}_{train_bs}\"\n",
        "    wandb.init(config=hparams,\n",
        "              project=\"Naive_CNN\",\n",
        "              entity=\"dsa4212-project\",\n",
        "              name=run_id,\n",
        "              )\n",
        "\n",
        "    # avalable GPU\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # get dataset\n",
        "    train, test = dataset.get_dataset(hparams[\"detaset\"])\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "                    train,\n",
        "                    batch_size=hparams[\"train_batch_size\"],\n",
        "                    shuffle=True,\n",
        "                    num_workers=2)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "                    test,\n",
        "                    batch_size=hparams[\"eval_batch_size\"],\n",
        "                    shuffle=False,\n",
        "                    num_workers=2)\n",
        "\n",
        "    hparams[\"n_classes\"] = len(train.classes)\n",
        "    hparams[\"input_shape\"] = train[0][0].shape\n",
        "\n",
        "    # get model\n",
        "    model = model_store.get_model(hparams).to(device)\n",
        "\n",
        "    # get optimizer\n",
        "    if \"SGD\" in hparams[\"optimizer\"]:\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=hparams[\"lr\"], momentum=hparams[\"momentum\"])\n",
        "    elif hparams[\"optimizer\"] == \"Adam\":\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=hparams[\"lr\"])\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"Training Started\")\n",
        "    # training phase\n",
        "    steps_per_epoch = len(train)/hparams['train_batch_size']\n",
        "    history = []\n",
        "    steps = 0\n",
        "    for epoch in range(hparams[\"epochs\"]):\n",
        "\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            steps += 1\n",
        "            results = {\n",
        "                      'step': steps,\n",
        "                      'epoch': steps / steps_per_epoch,\n",
        "                  }\n",
        "\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = F.cross_entropy(model(images), labels)\n",
        "            # history.append(loss.item())\n",
        "            results['loss'] = loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if i % hparams[\"checkpoint\"] == 0 or i == len(train_loader) - 1:\n",
        "                correct = 0\n",
        "                total = 0\n",
        "                with torch.no_grad():\n",
        "                  for data in test_loader:\n",
        "                      images, labels = data\n",
        "                      images, labels = images.to(device), labels.to(device)\n",
        "                      logits = model(images)\n",
        "                      pred_label = logits.argmax(dim=1)\n",
        "                      total += labels.size(0)\n",
        "                      correct += (pred_label == labels).sum().item()\n",
        "                  results['accuracy'] = 100 * correct / total\n",
        "\n",
        "            wandb.log(results)\n",
        "\n",
        "    print(\"Training Done\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
